#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è§†è§‰å¯¹è¯ç¤ºä¾‹
"""

import sys
import torch
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.api.chat_api import ChatAPI


def generate_mock_image(width=64, height=64, channels=3, image_type="random"):
    """
    ç”Ÿæˆæ¨¡æ‹Ÿå›¾åƒæ•°æ®
    
    Args:
        width: å›¾åƒå®½åº¦
        height: å›¾åƒé«˜åº¦
        channels: é€šé“æ•°
        image_type: å›¾åƒç±»å‹
    
    Returns:
        torch.Tensor: å›¾åƒå¼ é‡
    """
    if image_type == "gradient":
        # ç”Ÿæˆæ¸å˜å›¾åƒ
        image = torch.zeros(channels, height, width)
        for i in range(height):
            for j in range(width):
                image[:, i, j] = torch.tensor([i/height, j/width, (i+j)/(height+width)])
    
    elif image_type == "checkerboard":
        # ç”Ÿæˆæ£‹ç›˜å›¾åƒ
        image = torch.zeros(channels, height, width)
        for i in range(height):
            for j in range(width):
                if (i // 8 + j // 8) % 2 == 0:
                    image[:, i, j] = 1.0
    
    elif image_type == "circles":
        # ç”Ÿæˆåœ†å½¢å›¾æ¡ˆ
        image = torch.zeros(channels, height, width)
        center_x, center_y = width // 2, height // 2
        for i in range(height):
            for j in range(width):
                distance = ((i - center_y) ** 2 + (j - center_x) ** 2) ** 0.5
                if distance < min(width, height) // 4:
                    image[:, i, j] = 1.0
    
    else:  # random
        # ç”Ÿæˆéšæœºå›¾åƒ
        image = torch.randn(channels, height, width)
        image = torch.clamp(image, 0, 1)
    
    return image


def generate_mock_video(frames=8, width=64, height=64, channels=3, video_type="moving_circle"):
    """
    ç”Ÿæˆæ¨¡æ‹Ÿè§†é¢‘æ•°æ®
    
    Args:
        frames: å¸§æ•°
        width: è§†é¢‘å®½åº¦
        height: è§†é¢‘é«˜åº¦
        channels: é€šé“æ•°
        video_type: è§†é¢‘ç±»å‹
    
    Returns:
        torch.Tensor: è§†é¢‘å¼ é‡
    """
    video = torch.zeros(channels, frames, height, width)
    
    if video_type == "moving_circle":
        # ç”Ÿæˆç§»åŠ¨åœ†å½¢è§†é¢‘
        for frame in range(frames):
            # åœ†å¿ƒä½ç½®éšæ—¶é—´å˜åŒ–
            center_x = int(width * (0.2 + 0.6 * frame / frames))
            center_y = height // 2
            
            for i in range(height):
                for j in range(width):
                    distance = ((i - center_y) ** 2 + (j - center_x) ** 2) ** 0.5
                    if distance < min(width, height) // 6:
                        video[:, frame, i, j] = 1.0
    
    elif video_type == "rotating_pattern":
        # ç”Ÿæˆæ—‹è½¬å›¾æ¡ˆè§†é¢‘
        center_x, center_y = width // 2, height // 2
        for frame in range(frames):
            angle = 2 * np.pi * frame / frames
            for i in range(height):
                for j in range(width):
                    # æ—‹è½¬åæ ‡
                    x = j - center_x
                    y = i - center_y
                    rotated_x = x * np.cos(angle) - y * np.sin(angle)
                    rotated_y = x * np.sin(angle) + y * np.cos(angle)
                    
                    if abs(rotated_x) < 5 or abs(rotated_y) < 5:
                        video[:, frame, i, j] = 1.0
    
    else:  # random
        # ç”Ÿæˆéšæœºè§†é¢‘
        video = torch.randn(channels, frames, height, width)
        video = torch.clamp(video, 0, 1)
    
    return video


def simulate_different_visual_content():
    """æ¨¡æ‹Ÿä¸åŒç±»å‹çš„è§†è§‰å†…å®¹"""
    visual_content = {
        "é£æ™¯å›¾ç‰‡": {
            "data": generate_mock_image(image_type="gradient"),
            "type": "image",
            "description": "è¿™æ˜¯ä¸€å¼ é£æ™¯å›¾ç‰‡"
        },
        "å‡ ä½•å›¾æ¡ˆ": {
            "data": generate_mock_image(image_type="checkerboard"),
            "type": "image",
            "description": "è¿™æ˜¯å‡ ä½•å›¾æ¡ˆ"
        },
        "åœ†å½¢å›¾åƒ": {
            "data": generate_mock_image(image_type="circles"),
            "type": "image",
            "description": "è¿™æ˜¯åŒ…å«åœ†å½¢çš„å›¾åƒ"
        },
        "ç§»åŠ¨ç‰©ä½“è§†é¢‘": {
            "data": generate_mock_video(video_type="moving_circle"),
            "type": "video",
            "description": "è¿™æ˜¯ä¸€ä¸ªç§»åŠ¨ç‰©ä½“çš„è§†é¢‘"
        },
        "æ—‹è½¬å›¾æ¡ˆè§†é¢‘": {
            "data": generate_mock_video(video_type="rotating_pattern"),
            "type": "video",
            "description": "è¿™æ˜¯ä¸€ä¸ªæ—‹è½¬å›¾æ¡ˆçš„è§†é¢‘"
        }
    }
    return visual_content


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ‘ï¸ å¤šæ¨¡æ€AIè§†è§‰å¯¹è¯ç¤ºä¾‹")
    print("=" * 50)
    
    # æ¨¡å‹è·¯å¾„
    model_path = "checkpoints/multimodal_chat_model.pth"
    
    try:
        # åˆå§‹åŒ–èŠå¤©API
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        chat_api = ChatAPI(model_path)
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        model_info = chat_api.get_model_info()
        print(f"\næ¨¡å‹ä¿¡æ¯:")
        print(f"  æ¨¡å‹åç§°: {model_info['model_name']}")
        print(f"  å‚æ•°é‡: {model_info['total_parameters']:,}")
        print(f"  è®¾å¤‡: {model_info['device']}")
        
        # ç”Ÿæˆä¸åŒç±»å‹çš„è§†è§‰å†…å®¹
        visual_samples = simulate_different_visual_content()
        
        print("\nå¯ç”¨çš„è§†è§‰å†…å®¹:")
        for i, (content_type, info) in enumerate(visual_samples.items(), 1):
            print(f"  {i}. {content_type} ({info['type']})")
        
        print("\nå¼€å§‹è§†è§‰å¯¹è¯ (è¾“å…¥æ•°å­—é€‰æ‹©å†…å®¹, 'quit' é€€å‡º):")
        print("-" * 50)
        
        while True:
            # è·å–ç”¨æˆ·é€‰æ‹©
            user_input = input("\nè¯·é€‰æ‹©è§†è§‰å†…å®¹ (1-5) æˆ–è¾“å…¥é—®é¢˜: ").strip()
            
            # æ£€æŸ¥é€€å‡ºå‘½ä»¤
            if user_input.lower() == 'quit':
                print("å†è§ï¼")
                break
            
            # è·³è¿‡ç©ºè¾“å…¥
            if not user_input:
                continue
            
            try:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­—é€‰æ‹©
                if user_input.isdigit():
                    choice = int(user_input)
                    if 1 <= choice <= len(visual_samples):
                        content_name = list(visual_samples.keys())[choice - 1]
                        content_data = visual_samples[content_name]
                        
                        print(f"\næ­£åœ¨åˆ†æ {content_name}...")
                        
                        # å‘é€è§†è§‰æ•°æ®è¿›è¡Œåˆ†æ
                        result = chat_api.chat_vision(
                            visual_data=content_data["data"],
                            text_context=f"è¯·åˆ†æè¿™ä¸ª{content_data['type']}ï¼Œ{content_data['description']}",
                            is_video=(content_data["type"] == "video"),
                            max_length=100,
                            temperature=0.7
                        )
                        
                        if result['success']:
                            print(f"AIåˆ†æ: {result['response']}")
                        else:
                            print(f"é”™è¯¯: {result['error']}")
                    else:
                        print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1-5ä¹‹é—´çš„æ•°å­—")
                
                else:
                    # ç”¨æˆ·è¾“å…¥çš„æ˜¯é—®é¢˜ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªè§†è§‰å†…å®¹
                    content_name = list(visual_samples.keys())[0]  # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ª
                    content_data = visual_samples[content_name]
                    
                    print(f"\næ­£åœ¨ç»“åˆè§†è§‰å†…å®¹å›ç­”é—®é¢˜...")
                    
                    result = chat_api.chat_vision(
                        visual_data=content_data["data"],
                        text_context=user_input,
                        is_video=(content_data["type"] == "video"),
                        max_length=100,
                        temperature=0.8
                    )
                    
                    if result['success']:
                        print(f"AIå›å¤: {result['response']}")
                    else:
                        print(f"é”™è¯¯: {result['error']}")
                        
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—æˆ–é—®é¢˜")
            except Exception as e:
                print(f"å‘ç”Ÿé”™è¯¯: {e}")
        
        # æ˜¾ç¤ºå¯¹è¯å†å²
        history = chat_api.get_conversation_history()
        if history:
            print(f"\næœ¬æ¬¡å¯¹è¯å…± {len(history)} è½®:")
            for i, item in enumerate(history, 1):
                print(f"{i}. ç”¨æˆ·: {item['user']}")
                print(f"   AI: {item['ai']}")
    
    except FileNotFoundError:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {model_path}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡å‹æ–‡ä»¶")
    except Exception as e:
        print(f"åˆå§‹åŒ–å¤±è´¥: {e}")


def demo_visual_processing():
    """æ¼”ç¤ºè§†è§‰å¤„ç†åŠŸèƒ½"""
    print("\nğŸ“¸ è§†è§‰å¤„ç†æ¼”ç¤º")
    print("-" * 30)
    
    # ç”Ÿæˆä¸åŒç±»å‹çš„è§†è§‰å†…å®¹
    image = generate_mock_image(64, 64, 3, "gradient")
    video = generate_mock_video(8, 64, 64, 3, "moving_circle")
    
    print(f"å›¾åƒå½¢çŠ¶: {image.shape}")
    print(f"è§†é¢‘å½¢çŠ¶: {video.shape}")
    
    # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯
    print(f"\nå›¾åƒç»Ÿè®¡:")
    print(f"  å‡å€¼: {image.mean():.4f}")
    print(f"  æ ‡å‡†å·®: {image.std():.4f}")
    print(f"  æœ€å°å€¼: {image.min():.4f}")
    print(f"  æœ€å¤§å€¼: {image.max():.4f}")
    
    print(f"\nè§†é¢‘ç»Ÿè®¡:")
    print(f"  å‡å€¼: {video.mean():.4f}")
    print(f"  æ ‡å‡†å·®: {video.std():.4f}")
    print(f"  æœ€å°å€¼: {video.min():.4f}")
    print(f"  æœ€å¤§å€¼: {video.max():.4f}")
    
    # åˆ†æè§†é¢‘ä¸­çš„è¿åŠ¨
    frame_diff = torch.abs(video[:, 1:] - video[:, :-1]).mean()
    print(f"  å¸§é—´å·®å¼‚: {frame_diff:.4f}")


def demo_multimodal_interaction():
    """æ¼”ç¤ºå¤šæ¨¡æ€äº¤äº’"""
    print("\nğŸ”„ å¤šæ¨¡æ€äº¤äº’æ¼”ç¤º")
    print("-" * 30)
    
    # åˆ›å»ºåŒ…å«æ–‡æœ¬ã€å›¾åƒçš„å¤šæ¨¡æ€è¾“å…¥
    text_context = "è¯·æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å†…å®¹"
    image_data = generate_mock_image(image_type="circles")
    
    print(f"æ–‡æœ¬è¾“å…¥: {text_context}")
    print(f"å›¾åƒæ•°æ®å½¢çŠ¶: {image_data.shape}")
    
    # è¿™é‡Œå¯ä»¥å±•ç¤ºå¦‚ä½•å°†å¤šç§æ¨¡æ€ç»“åˆä½¿ç”¨
    print("å¤šæ¨¡æ€è¾“å…¥å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å‘é€ç»™AIæ¨¡å‹è¿›è¡Œå¤„ç†")


if __name__ == '__main__':
    # è¿è¡Œä¸»ç¨‹åº
    main()
    
    # è¿è¡Œè§†è§‰å¤„ç†æ¼”ç¤º
    demo_visual_processing()
    
    # è¿è¡Œå¤šæ¨¡æ€äº¤äº’æ¼”ç¤º
    demo_multimodal_interaction()
