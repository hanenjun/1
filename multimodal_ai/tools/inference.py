#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨ç†è„šæœ¬
"""

import torch
import numpy as np
import logging
import sys
from pathlib import Path
import argparse
import json
import time
from typing import Optional, Union

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.api.chat_api import ChatAPI

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class InferenceEngine:
    """æ¨ç†å¼•æ“"""
    
    def __init__(self, model_path: str, config_path: str = None, device: str = 'auto'):
        self.chat_api = ChatAPI(
            model_path=model_path,
            config_path=config_path,
            device=device
        )
        logger.info("æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def text_inference(self, 
                      text: str,
                      max_length: int = 50,
                      temperature: float = 1.0,
                      top_k: int = 50,
                      top_p: float = 0.9) -> dict:
        """æ–‡æœ¬æ¨ç†"""
        logger.info(f"æ–‡æœ¬æ¨ç†: {text[:50]}...")
        
        start_time = time.time()
        result = self.chat_api.chat_text(
            text_input=text,
            max_length=max_length,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p
        )
        end_time = time.time()
        
        result['inference_time'] = end_time - start_time
        return result
    
    def audio_inference(self,
                       audio_file: str,
                       text_context: Optional[str] = None,
                       max_length: int = 50,
                       temperature: float = 1.0) -> dict:
        """éŸ³é¢‘æ¨ç†"""
        logger.info(f"éŸ³é¢‘æ¨ç†: {audio_file}")
        
        # è¿™é‡Œåº”è¯¥åŠ è½½çœŸå®çš„éŸ³é¢‘æ–‡ä»¶å¹¶æå–ç‰¹å¾
        # ç›®å‰ä½¿ç”¨éšæœºç‰¹å¾ä½œä¸ºç¤ºä¾‹
        audio_features = torch.randn(100, 128)  # [seq_len, feature_dim]
        
        start_time = time.time()
        result = self.chat_api.chat_audio(
            audio_features=audio_features,
            text_context=text_context,
            max_length=max_length,
            temperature=temperature
        )
        end_time = time.time()
        
        result['inference_time'] = end_time - start_time
        return result
    
    def vision_inference(self,
                        image_file: str,
                        text_context: Optional[str] = None,
                        is_video: bool = False,
                        max_length: int = 50,
                        temperature: float = 1.0) -> dict:
        """è§†è§‰æ¨ç†"""
        logger.info(f"è§†è§‰æ¨ç†: {image_file}")
        
        # è¿™é‡Œåº”è¯¥åŠ è½½çœŸå®çš„å›¾åƒ/è§†é¢‘æ–‡ä»¶
        # ç›®å‰ä½¿ç”¨éšæœºæ•°æ®ä½œä¸ºç¤ºä¾‹
        if is_video:
            vision_data = torch.randn(3, 8, 64, 64)  # [channels, frames, height, width]
        else:
            vision_data = torch.randn(3, 64, 64)  # [channels, height, width]
        
        start_time = time.time()
        result = self.chat_api.chat_vision(
            image_or_video=vision_data,
            text_context=text_context,
            is_video=is_video,
            max_length=max_length,
            temperature=temperature
        )
        end_time = time.time()
        
        result['inference_time'] = end_time - start_time
        return result
    
    def interactive_chat(self):
        """äº¤äº’å¼èŠå¤©"""
        print("ğŸ¤– å¤šæ¨¡æ€AIèŠå¤©ç³»ç»Ÿ")
        print("è¾“å…¥ 'quit' é€€å‡ºï¼Œ'clear' æ¸…ç©ºå†å²ï¼Œ'help' æŸ¥çœ‹å¸®åŠ©")
        print("-" * 50)
        
        while True:
            try:
                user_input = input("\nç”¨æˆ·: ").strip()
                
                if user_input.lower() == 'quit':
                    print("å†è§ï¼")
                    break
                elif user_input.lower() == 'clear':
                    self.chat_api.clear_conversation_history()
                    print("å¯¹è¯å†å²å·²æ¸…ç©º")
                    continue
                elif user_input.lower() == 'help':
                    self._print_help()
                    continue
                elif not user_input:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹æ®Šå‘½ä»¤
                if user_input.startswith('/'):
                    self._handle_command(user_input)
                    continue
                
                # æ™®é€šæ–‡æœ¬å¯¹è¯
                result = self.text_inference(user_input)
                
                if result['success']:
                    print(f"AI: {result['response']}")
                    print(f"(æ¨ç†æ—¶é—´: {result['inference_time']:.3f}s)")
                else:
                    print(f"é”™è¯¯: {result['error']}")
                    
            except KeyboardInterrupt:
                print("\n\nå†è§ï¼")
                break
            except Exception as e:
                print(f"å‘ç”Ÿé”™è¯¯: {e}")
    
    def _handle_command(self, command: str):
        """å¤„ç†ç‰¹æ®Šå‘½ä»¤"""
        parts = command[1:].split()
        cmd = parts[0].lower()
        
        if cmd == 'audio':
            if len(parts) < 2:
                print("ç”¨æ³•: /audio <éŸ³é¢‘æ–‡ä»¶è·¯å¾„> [æ–‡æœ¬ä¸Šä¸‹æ–‡]")
                return
            
            audio_file = parts[1]
            text_context = ' '.join(parts[2:]) if len(parts) > 2 else None
            
            result = self.audio_inference(audio_file, text_context)
            if result['success']:
                print(f"AI: {result['response']}")
                print(f"(æ¨ç†æ—¶é—´: {result['inference_time']:.3f}s)")
            else:
                print(f"é”™è¯¯: {result['error']}")
        
        elif cmd == 'image':
            if len(parts) < 2:
                print("ç”¨æ³•: /image <å›¾åƒæ–‡ä»¶è·¯å¾„> [æ–‡æœ¬ä¸Šä¸‹æ–‡]")
                return
            
            image_file = parts[1]
            text_context = ' '.join(parts[2:]) if len(parts) > 2 else None
            
            result = self.vision_inference(image_file, text_context, is_video=False)
            if result['success']:
                print(f"AI: {result['response']}")
                print(f"(æ¨ç†æ—¶é—´: {result['inference_time']:.3f}s)")
            else:
                print(f"é”™è¯¯: {result['error']}")
        
        elif cmd == 'video':
            if len(parts) < 2:
                print("ç”¨æ³•: /video <è§†é¢‘æ–‡ä»¶è·¯å¾„> [æ–‡æœ¬ä¸Šä¸‹æ–‡]")
                return
            
            video_file = parts[1]
            text_context = ' '.join(parts[2:]) if len(parts) > 2 else None
            
            result = self.vision_inference(video_file, text_context, is_video=True)
            if result['success']:
                print(f"AI: {result['response']}")
                print(f"(æ¨ç†æ—¶é—´: {result['inference_time']:.3f}s)")
            else:
                print(f"é”™è¯¯: {result['error']}")
        
        elif cmd == 'history':
            history = self.chat_api.get_conversation_history()
            if history:
                print("\nå¯¹è¯å†å²:")
                for i, item in enumerate(history, 1):
                    print(f"{i}. ç”¨æˆ·: {item['user']}")
                    print(f"   AI: {item['ai']}")
            else:
                print("æš‚æ— å¯¹è¯å†å²")
        
        elif cmd == 'info':
            info = self.chat_api.get_model_info()
            print(f"\næ¨¡å‹ä¿¡æ¯:")
            print(f"  æ¨¡å‹åç§°: {info['model_name']}")
            print(f"  å‚æ•°é‡: {info['total_parameters']:,}")
            print(f"  è¯æ±‡è¡¨å¤§å°: {info['vocab_size']}")
            print(f"  è®¾å¤‡: {info['device']}")
        
        else:
            print(f"æœªçŸ¥å‘½ä»¤: {cmd}")
            self._print_help()
    
    def _print_help(self):
        """æ‰“å°å¸®åŠ©ä¿¡æ¯"""
        help_text = """
å¯ç”¨å‘½ä»¤:
  quit          - é€€å‡ºç¨‹åº
  clear         - æ¸…ç©ºå¯¹è¯å†å²
  help          - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
  /audio <æ–‡ä»¶> [ä¸Šä¸‹æ–‡] - éŸ³é¢‘æ¨ç†
  /image <æ–‡ä»¶> [ä¸Šä¸‹æ–‡] - å›¾åƒæ¨ç†
  /video <æ–‡ä»¶> [ä¸Šä¸‹æ–‡] - è§†é¢‘æ¨ç†
  /history      - æŸ¥çœ‹å¯¹è¯å†å²
  /info         - æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯

ç¤ºä¾‹:
  ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±
  /audio audio.wav è¿™æ˜¯ä»€ä¹ˆå£°éŸ³ï¼Ÿ
  /image photo.jpg æè¿°è¿™å¼ å›¾ç‰‡
  /video video.mp4 è¿™ä¸ªè§†é¢‘åœ¨åšä»€ä¹ˆï¼Ÿ
        """
        print(help_text)
    
    def batch_inference(self, input_file: str, output_file: str):
        """æ‰¹é‡æ¨ç†"""
        logger.info(f"å¼€å§‹æ‰¹é‡æ¨ç†: {input_file} -> {output_file}")
        
        # è¯»å–è¾“å…¥æ–‡ä»¶
        with open(input_file, 'r', encoding='utf-8') as f:
            inputs = json.load(f)
        
        results = []
        
        for i, item in enumerate(inputs):
            logger.info(f"å¤„ç†ç¬¬ {i+1}/{len(inputs)} ä¸ªæ ·æœ¬")
            
            input_type = item.get('type', 'text')
            
            if input_type == 'text':
                result = self.text_inference(
                    text=item['text'],
                    max_length=item.get('max_length', 50),
                    temperature=item.get('temperature', 1.0)
                )
            elif input_type == 'audio':
                result = self.audio_inference(
                    audio_file=item['audio_file'],
                    text_context=item.get('text_context'),
                    max_length=item.get('max_length', 50),
                    temperature=item.get('temperature', 1.0)
                )
            elif input_type == 'image':
                result = self.vision_inference(
                    image_file=item['image_file'],
                    text_context=item.get('text_context'),
                    is_video=False,
                    max_length=item.get('max_length', 50),
                    temperature=item.get('temperature', 1.0)
                )
            elif input_type == 'video':
                result = self.vision_inference(
                    image_file=item['video_file'],
                    text_context=item.get('text_context'),
                    is_video=True,
                    max_length=item.get('max_length', 50),
                    temperature=item.get('temperature', 1.0)
                )
            else:
                result = {'success': False, 'error': f'æœªçŸ¥è¾“å…¥ç±»å‹: {input_type}'}
            
            results.append({
                'input': item,
                'output': result
            })
        
        # ä¿å­˜ç»“æœ
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ‰¹é‡æ¨ç†å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤šæ¨¡æ€AIæ¨ç†')
    parser.add_argument('--model-path', required=True, help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--config-path', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--device', default='auto', help='è®¾å¤‡ (cpu/cuda/auto)')
    
    # æ¨ç†æ¨¡å¼
    parser.add_argument('--mode', choices=['interactive', 'text', 'audio', 'image', 'video', 'batch'],
                       default='interactive', help='æ¨ç†æ¨¡å¼')
    
    # æ–‡æœ¬æ¨ç†å‚æ•°
    parser.add_argument('--text', help='è¾“å…¥æ–‡æœ¬')
    parser.add_argument('--max-length', type=int, default=50, help='æœ€å¤§ç”Ÿæˆé•¿åº¦')
    parser.add_argument('--temperature', type=float, default=1.0, help='æ¸©åº¦å‚æ•°')
    parser.add_argument('--top-k', type=int, default=50, help='Top-Ké‡‡æ ·')
    parser.add_argument('--top-p', type=float, default=0.9, help='Top-Pé‡‡æ ·')
    
    # å¤šæ¨¡æ€æ¨ç†å‚æ•°
    parser.add_argument('--audio-file', help='éŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--image-file', help='å›¾åƒæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--video-file', help='è§†é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--text-context', help='æ–‡æœ¬ä¸Šä¸‹æ–‡')
    
    # æ‰¹é‡æ¨ç†å‚æ•°
    parser.add_argument('--input-file', help='æ‰¹é‡æ¨ç†è¾“å…¥æ–‡ä»¶')
    parser.add_argument('--output-file', help='æ‰¹é‡æ¨ç†è¾“å‡ºæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¨ç†å¼•æ“
    engine = InferenceEngine(args.model_path, args.config_path, args.device)
    
    # æ ¹æ®æ¨¡å¼æ‰§è¡Œæ¨ç†
    if args.mode == 'interactive':
        engine.interactive_chat()
    
    elif args.mode == 'text':
        if not args.text:
            print("é”™è¯¯: æ–‡æœ¬æ¨¡å¼éœ€è¦ --text å‚æ•°")
            return
        
        result = engine.text_inference(
            text=args.text,
            max_length=args.max_length,
            temperature=args.temperature,
            top_k=args.top_k,
            top_p=args.top_p
        )
        
        if result['success']:
            print(f"è¾“å…¥: {args.text}")
            print(f"è¾“å‡º: {result['response']}")
            print(f"æ¨ç†æ—¶é—´: {result['inference_time']:.3f}s")
        else:
            print(f"é”™è¯¯: {result['error']}")
    
    elif args.mode == 'audio':
        if not args.audio_file:
            print("é”™è¯¯: éŸ³é¢‘æ¨¡å¼éœ€è¦ --audio-file å‚æ•°")
            return
        
        result = engine.audio_inference(
            audio_file=args.audio_file,
            text_context=args.text_context,
            max_length=args.max_length,
            temperature=args.temperature
        )
        
        if result['success']:
            print(f"éŸ³é¢‘æ–‡ä»¶: {args.audio_file}")
            print(f"æ–‡æœ¬ä¸Šä¸‹æ–‡: {args.text_context or 'æ— '}")
            print(f"è¾“å‡º: {result['response']}")
            print(f"æ¨ç†æ—¶é—´: {result['inference_time']:.3f}s")
        else:
            print(f"é”™è¯¯: {result['error']}")
    
    elif args.mode == 'image':
        if not args.image_file:
            print("é”™è¯¯: å›¾åƒæ¨¡å¼éœ€è¦ --image-file å‚æ•°")
            return
        
        result = engine.vision_inference(
            image_file=args.image_file,
            text_context=args.text_context,
            is_video=False,
            max_length=args.max_length,
            temperature=args.temperature
        )
        
        if result['success']:
            print(f"å›¾åƒæ–‡ä»¶: {args.image_file}")
            print(f"æ–‡æœ¬ä¸Šä¸‹æ–‡: {args.text_context or 'æ— '}")
            print(f"è¾“å‡º: {result['response']}")
            print(f"æ¨ç†æ—¶é—´: {result['inference_time']:.3f}s")
        else:
            print(f"é”™è¯¯: {result['error']}")
    
    elif args.mode == 'video':
        if not args.video_file:
            print("é”™è¯¯: è§†é¢‘æ¨¡å¼éœ€è¦ --video-file å‚æ•°")
            return
        
        result = engine.vision_inference(
            image_file=args.video_file,
            text_context=args.text_context,
            is_video=True,
            max_length=args.max_length,
            temperature=args.temperature
        )
        
        if result['success']:
            print(f"è§†é¢‘æ–‡ä»¶: {args.video_file}")
            print(f"æ–‡æœ¬ä¸Šä¸‹æ–‡: {args.text_context or 'æ— '}")
            print(f"è¾“å‡º: {result['response']}")
            print(f"æ¨ç†æ—¶é—´: {result['inference_time']:.3f}s")
        else:
            print(f"é”™è¯¯: {result['error']}")
    
    elif args.mode == 'batch':
        if not args.input_file or not args.output_file:
            print("é”™è¯¯: æ‰¹é‡æ¨¡å¼éœ€è¦ --input-file å’Œ --output-file å‚æ•°")
            return
        
        engine.batch_inference(args.input_file, args.output_file)


if __name__ == '__main__':
    main()
